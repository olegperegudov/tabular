{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "bd47a18303be29f6974e8153b6e52f9a90ee51d979b528b256bf5f0592078974"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@invest_gs/a-simple-classification-challenge-with-lightgbm-kaggle-competition-e12467cfec96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data sets\n",
    "df_train = pd.read_csv(r\"data\\train.csv\")\n",
    "df_test = pd.read_csv(r\"data\\test.csv\")\n",
    "sample_submission = pd.read_csv(r\"data\\sample_submission.csv\")\n",
    "# create splitter feature we will need later to split train and val back\n",
    "df_test['target'] = -1\n",
    "# Create a merged data set and review initial information\n",
    "df_comb = pd.concat([df_train, df_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Check missing values\n",
    "df_comb.isna().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dtypes before: int64      49\n",
      "float64    10\n",
      "dtype: int64\n",
      "\n",
      "dtypes after: uint8      218\n",
      "int64       17\n",
      "float64     10\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Get the data types\n",
    "print(f'dtypes before: {df_comb.dtypes.value_counts()}\\n')\n",
    "\n",
    "# Set the ID col as index\n",
    "df_comb.set_index('id', inplace = True)\n",
    "df_comb.sort_index(inplace=True)\n",
    "\n",
    "# Create dummies for categorical and binary values (For LGBM train remove it)\n",
    "df_comb = pd.get_dummies(df_comb, columns=[c for c in df_comb if c.endswith('bin') or c.endswith('cat')])\n",
    "\n",
    "# Get the data types again to check our transformation\n",
    "print(f'dtypes after: {df_comb.dtypes.value_counts()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df_comb back to train and test\n",
    "df_train = df_comb.loc[df_comb[\"target\"].isin([0,1])]\n",
    "df_test = df_comb.loc[df_comb[\"target\"].isin([-1])].drop('target', 1)\n",
    "\n",
    "# Create x_train and y_train\n",
    "x = df_train.drop(\"target\", 1)\n",
    "y = df_train[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the scaler (try all on cross val)\n",
    "#scaler = RobustScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the x_train set\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "x = pd.DataFrame(x_scaled, index=x.index, columns=x.columns)\n",
    "\n",
    "# Scale the x_test set\n",
    "test_scaled = scaler.transform(df_test)\n",
    "test = pd.DataFrame(test_scaled, index=df_test.index, columns=df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our training sample into train and test, leave 20% for test \n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([x_train, y_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    458809\n",
       "1     17360\n",
       "Name: target, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0    458809\n1    458809\nName: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Separate minority and majority classes\n",
    "no_target = df[df.target==0]\n",
    "yes_target = df[df.target==1]\n",
    "\n",
    "yes_target_up = yes_target.sample(no_target.shape[0],\n",
    "                                  replace=True, # sample with replacement\n",
    "                                  random_state=1)\n",
    "\n",
    "# Combine minority and downsampled majority\n",
    "upsampled = pd.concat([yes_target_up, no_target], axis=0)\n",
    "\n",
    "# Checking counts\n",
    "print(upsampled.target.value_counts())\n",
    "\n",
    "# Create training set again\n",
    "x_train = upsampled.drop('target', axis=1)\n",
    "y_train = upsampled.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Oleg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightgbm\\basic.py:1551: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "C:\\Users\\Oleg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lightgbm\\basic.py:1554: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is []\n",
      "  warnings.warn('categorical_feature in Dataset is overridden.\\n'\n",
      "[LightGBM] [Warning] objective is set=binary, application=binary will be ignored. Current value: objective=binary\n",
      "[LightGBM] [Warning] objective is set=binary, application=binary will be ignored. Current value: objective=binary\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.560287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] objective is set=binary, application=binary will be ignored. Current value: objective=binary\n",
      "[1]\tvalid_0's auc: 0.605305\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's auc: 0.613468\n",
      "[3]\tvalid_0's auc: 0.620565\n",
      "[4]\tvalid_0's auc: 0.622787\n",
      "[5]\tvalid_0's auc: 0.624079\n",
      "[6]\tvalid_0's auc: 0.62576\n",
      "[7]\tvalid_0's auc: 0.625417\n",
      "[8]\tvalid_0's auc: 0.62702\n",
      "[9]\tvalid_0's auc: 0.626957\n",
      "[10]\tvalid_0's auc: 0.62799\n",
      "[11]\tvalid_0's auc: 0.627978\n",
      "[12]\tvalid_0's auc: 0.628339\n",
      "[13]\tvalid_0's auc: 0.6286\n",
      "[14]\tvalid_0's auc: 0.62876\n",
      "[15]\tvalid_0's auc: 0.628821\n",
      "[16]\tvalid_0's auc: 0.629042\n",
      "[17]\tvalid_0's auc: 0.629433\n",
      "[18]\tvalid_0's auc: 0.629982\n",
      "[19]\tvalid_0's auc: 0.630446\n",
      "[20]\tvalid_0's auc: 0.630594\n",
      "[21]\tvalid_0's auc: 0.630588\n",
      "[22]\tvalid_0's auc: 0.631022\n",
      "[23]\tvalid_0's auc: 0.631077\n",
      "[24]\tvalid_0's auc: 0.631414\n",
      "[25]\tvalid_0's auc: 0.631774\n",
      "[26]\tvalid_0's auc: 0.631811\n",
      "[27]\tvalid_0's auc: 0.631607\n",
      "[28]\tvalid_0's auc: 0.631604\n",
      "[29]\tvalid_0's auc: 0.631779\n",
      "[30]\tvalid_0's auc: 0.631969\n",
      "[31]\tvalid_0's auc: 0.63228\n",
      "[32]\tvalid_0's auc: 0.632353\n",
      "[33]\tvalid_0's auc: 0.632609\n",
      "[34]\tvalid_0's auc: 0.632827\n",
      "[35]\tvalid_0's auc: 0.632995\n",
      "[36]\tvalid_0's auc: 0.633167\n",
      "[37]\tvalid_0's auc: 0.633275\n",
      "[38]\tvalid_0's auc: 0.633389\n",
      "[39]\tvalid_0's auc: 0.633533\n",
      "[40]\tvalid_0's auc: 0.633398\n",
      "[41]\tvalid_0's auc: 0.633332\n",
      "[42]\tvalid_0's auc: 0.63365\n",
      "[43]\tvalid_0's auc: 0.633701\n",
      "[44]\tvalid_0's auc: 0.633699\n",
      "[45]\tvalid_0's auc: 0.633871\n",
      "[46]\tvalid_0's auc: 0.633842\n",
      "[47]\tvalid_0's auc: 0.633992\n",
      "[48]\tvalid_0's auc: 0.634398\n",
      "[49]\tvalid_0's auc: 0.63438\n",
      "[50]\tvalid_0's auc: 0.634539\n",
      "[51]\tvalid_0's auc: 0.63484\n",
      "[52]\tvalid_0's auc: 0.635096\n",
      "[53]\tvalid_0's auc: 0.635076\n",
      "[54]\tvalid_0's auc: 0.635175\n",
      "[55]\tvalid_0's auc: 0.635358\n",
      "[56]\tvalid_0's auc: 0.635517\n",
      "[57]\tvalid_0's auc: 0.635592\n",
      "[58]\tvalid_0's auc: 0.635897\n",
      "[59]\tvalid_0's auc: 0.636004\n",
      "[60]\tvalid_0's auc: 0.636033\n",
      "[61]\tvalid_0's auc: 0.636147\n",
      "[62]\tvalid_0's auc: 0.636152\n",
      "[63]\tvalid_0's auc: 0.636305\n",
      "[64]\tvalid_0's auc: 0.636342\n",
      "[65]\tvalid_0's auc: 0.63632\n",
      "[66]\tvalid_0's auc: 0.63651\n",
      "[67]\tvalid_0's auc: 0.636555\n",
      "[68]\tvalid_0's auc: 0.636635\n",
      "[69]\tvalid_0's auc: 0.636834\n",
      "[70]\tvalid_0's auc: 0.637047\n",
      "[71]\tvalid_0's auc: 0.637258\n",
      "[72]\tvalid_0's auc: 0.637357\n",
      "[73]\tvalid_0's auc: 0.637468\n",
      "[74]\tvalid_0's auc: 0.637502\n",
      "[75]\tvalid_0's auc: 0.637476\n",
      "[76]\tvalid_0's auc: 0.637655\n",
      "[77]\tvalid_0's auc: 0.637681\n",
      "[78]\tvalid_0's auc: 0.637716\n",
      "[79]\tvalid_0's auc: 0.637836\n",
      "[80]\tvalid_0's auc: 0.637891\n",
      "[81]\tvalid_0's auc: 0.637769\n",
      "[82]\tvalid_0's auc: 0.63783\n",
      "[83]\tvalid_0's auc: 0.637888\n",
      "[84]\tvalid_0's auc: 0.637938\n",
      "[85]\tvalid_0's auc: 0.638133\n",
      "[86]\tvalid_0's auc: 0.63824\n",
      "[87]\tvalid_0's auc: 0.638415\n",
      "[88]\tvalid_0's auc: 0.638482\n",
      "[89]\tvalid_0's auc: 0.638482\n",
      "[90]\tvalid_0's auc: 0.638401\n",
      "[91]\tvalid_0's auc: 0.63839\n",
      "[92]\tvalid_0's auc: 0.638454\n",
      "[93]\tvalid_0's auc: 0.638522\n",
      "[94]\tvalid_0's auc: 0.63868\n",
      "[95]\tvalid_0's auc: 0.638682\n",
      "[96]\tvalid_0's auc: 0.63876\n",
      "[97]\tvalid_0's auc: 0.638761\n",
      "[98]\tvalid_0's auc: 0.638672\n",
      "[99]\tvalid_0's auc: 0.638671\n",
      "[100]\tvalid_0's auc: 0.638637\n",
      "[101]\tvalid_0's auc: 0.638656\n",
      "[102]\tvalid_0's auc: 0.638587\n",
      "[103]\tvalid_0's auc: 0.638621\n",
      "[104]\tvalid_0's auc: 0.63876\n",
      "[105]\tvalid_0's auc: 0.638733\n",
      "[106]\tvalid_0's auc: 0.63877\n",
      "[107]\tvalid_0's auc: 0.638651\n",
      "[108]\tvalid_0's auc: 0.638591\n",
      "[109]\tvalid_0's auc: 0.638606\n",
      "[110]\tvalid_0's auc: 0.638562\n",
      "[111]\tvalid_0's auc: 0.638532\n",
      "[112]\tvalid_0's auc: 0.638564\n",
      "[113]\tvalid_0's auc: 0.638525\n",
      "[114]\tvalid_0's auc: 0.638618\n",
      "[115]\tvalid_0's auc: 0.638753\n",
      "[116]\tvalid_0's auc: 0.638759\n",
      "[117]\tvalid_0's auc: 0.638688\n",
      "[118]\tvalid_0's auc: 0.63865\n",
      "[119]\tvalid_0's auc: 0.63861\n",
      "[120]\tvalid_0's auc: 0.638607\n",
      "[121]\tvalid_0's auc: 0.638644\n",
      "[122]\tvalid_0's auc: 0.6386\n",
      "[123]\tvalid_0's auc: 0.638524\n",
      "[124]\tvalid_0's auc: 0.638583\n",
      "[125]\tvalid_0's auc: 0.638566\n",
      "[126]\tvalid_0's auc: 0.638603\n",
      "[127]\tvalid_0's auc: 0.63858\n",
      "[128]\tvalid_0's auc: 0.638562\n",
      "[129]\tvalid_0's auc: 0.638556\n",
      "[130]\tvalid_0's auc: 0.638589\n",
      "[131]\tvalid_0's auc: 0.638539\n",
      "[132]\tvalid_0's auc: 0.638651\n",
      "[133]\tvalid_0's auc: 0.638587\n",
      "[134]\tvalid_0's auc: 0.638511\n",
      "[135]\tvalid_0's auc: 0.63855\n",
      "[136]\tvalid_0's auc: 0.638452\n",
      "[137]\tvalid_0's auc: 0.638495\n",
      "[138]\tvalid_0's auc: 0.63851\n",
      "[139]\tvalid_0's auc: 0.638453\n",
      "[140]\tvalid_0's auc: 0.638379\n",
      "[141]\tvalid_0's auc: 0.638427\n",
      "[142]\tvalid_0's auc: 0.638353\n",
      "[143]\tvalid_0's auc: 0.638349\n",
      "[144]\tvalid_0's auc: 0.638262\n",
      "[145]\tvalid_0's auc: 0.638284\n",
      "[146]\tvalid_0's auc: 0.638341\n",
      "[147]\tvalid_0's auc: 0.638425\n",
      "[148]\tvalid_0's auc: 0.63832\n",
      "[149]\tvalid_0's auc: 0.638375\n",
      "[150]\tvalid_0's auc: 0.638284\n",
      "[151]\tvalid_0's auc: 0.638319\n",
      "[152]\tvalid_0's auc: 0.638401\n",
      "[153]\tvalid_0's auc: 0.63839\n",
      "[154]\tvalid_0's auc: 0.638337\n",
      "[155]\tvalid_0's auc: 0.638337\n",
      "[156]\tvalid_0's auc: 0.638446\n",
      "[157]\tvalid_0's auc: 0.638462\n",
      "[158]\tvalid_0's auc: 0.638422\n",
      "[159]\tvalid_0's auc: 0.638465\n",
      "[160]\tvalid_0's auc: 0.638431\n",
      "[161]\tvalid_0's auc: 0.638382\n",
      "[162]\tvalid_0's auc: 0.638333\n",
      "[163]\tvalid_0's auc: 0.638311\n",
      "[164]\tvalid_0's auc: 0.638301\n",
      "[165]\tvalid_0's auc: 0.638256\n",
      "[166]\tvalid_0's auc: 0.638206\n",
      "[167]\tvalid_0's auc: 0.638167\n",
      "[168]\tvalid_0's auc: 0.638066\n",
      "[169]\tvalid_0's auc: 0.638084\n",
      "[170]\tvalid_0's auc: 0.637956\n",
      "[171]\tvalid_0's auc: 0.637983\n",
      "[172]\tvalid_0's auc: 0.637984\n",
      "[173]\tvalid_0's auc: 0.637922\n",
      "[174]\tvalid_0's auc: 0.637975\n",
      "[175]\tvalid_0's auc: 0.63789\n",
      "[176]\tvalid_0's auc: 0.637796\n",
      "[177]\tvalid_0's auc: 0.637767\n",
      "[178]\tvalid_0's auc: 0.637661\n",
      "[179]\tvalid_0's auc: 0.63764\n",
      "[180]\tvalid_0's auc: 0.63757\n",
      "[181]\tvalid_0's auc: 0.637641\n",
      "[182]\tvalid_0's auc: 0.637632\n",
      "[183]\tvalid_0's auc: 0.637657\n",
      "[184]\tvalid_0's auc: 0.637546\n",
      "[185]\tvalid_0's auc: 0.637526\n",
      "[186]\tvalid_0's auc: 0.637459\n",
      "[187]\tvalid_0's auc: 0.637358\n",
      "[188]\tvalid_0's auc: 0.637317\n",
      "[189]\tvalid_0's auc: 0.637322\n",
      "[190]\tvalid_0's auc: 0.637252\n",
      "[191]\tvalid_0's auc: 0.637226\n",
      "[192]\tvalid_0's auc: 0.637282\n",
      "[193]\tvalid_0's auc: 0.637215\n",
      "[194]\tvalid_0's auc: 0.637217\n",
      "[195]\tvalid_0's auc: 0.637335\n",
      "[196]\tvalid_0's auc: 0.637368\n",
      "[197]\tvalid_0's auc: 0.637316\n",
      "[198]\tvalid_0's auc: 0.637273\n",
      "[199]\tvalid_0's auc: 0.637255\n",
      "[200]\tvalid_0's auc: 0.63718\n",
      "[201]\tvalid_0's auc: 0.637224\n",
      "[202]\tvalid_0's auc: 0.637278\n",
      "[203]\tvalid_0's auc: 0.637222\n",
      "[204]\tvalid_0's auc: 0.637173\n",
      "[205]\tvalid_0's auc: 0.637221\n",
      "[206]\tvalid_0's auc: 0.637127\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid_0's auc: 0.63877\n"
     ]
    }
   ],
   "source": [
    "# LIGHT GBM\n",
    "\n",
    "# Indicate the categorical features for the LGBM classifier\n",
    "categorical_features = [col for col in x_train.columns if col.endswith('cat')]\n",
    "\n",
    "# Get the train and test data for the training sequence\n",
    "train_data = lgbm.Dataset(x_train, label=y_train, categorical_feature=categorical_features)\n",
    "valid_data = lgbm.Dataset(x_valid, label=y_valid)\n",
    "\n",
    "# Set the parameters for training\n",
    "parameters = {\n",
    "    'application': 'binary',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    #'is_unbalance': 'true',\n",
    "    'boosting': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# Train the classifier\n",
    "classifier = lgbm.train(parameters,\n",
    "                        train_data,\n",
    "                        valid_sets=valid_data,\n",
    "                        num_boost_round=5000,\n",
    "                        early_stopping_rounds=100)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "# preds = classifier.predict(test)\n",
    "preds = classifier.predict(test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc: 0.63877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission file\n",
    "my_pred_lgbm = pd.DataFrame({'id': test.index, 'target': preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(892816, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "my_pred_lgbm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSV file\n",
    "my_pred_lgbm.to_csv('pred_lgbm.csv', index=False)"
   ]
  }
 ]
}